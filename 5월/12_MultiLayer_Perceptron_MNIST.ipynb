{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings as wns\n",
    "wns.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 퍼셉트론 텐서플로우 모델을 이용해서 MNIST 데이터를 분류하는 다중 퍼셉트론을 구현한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# 깃헙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# MNIST 손글씨 실습을 위해 케라스에서 제공하는 MNIST 데이터 셋을 사용한다.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# 학습 데이터에는 총 60,000개의 샘플이 있고 테스트 데이턴에는 총 10,000개의 샘플이 있다.\n",
    "# MNIST 데이터는 총 28개의 행과 28개의 열을 갖는 픽셀 데이터이다. 각 픽셀을 흑백 사진과 같이 \n",
    "# 0부터 255까지의 그레이스케일을 가지고 있다.\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터(5만개)와 검증 데이터(1만개)를 분리한다.  \n",
    "학습 중간마다 검증 데이터로 모델의 성능을 측정하면 모델 학습이 제대로 진행되는지 검증 정확도를 알 수 있고 학습 정확도는 올라가는데 검증 정확도가 안 올라가거나 떨어질 경우 조기 종료를 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[50000:]   # 28행 28열로 구성된 검증 데이터\n",
    "x_train = x_train[:50000] # 28행 28열로 구성된 학습 데이터\n",
    "y_val = y_train[50000:]   # 검증 데이터 실제값\n",
    "y_train = y_train[:50000] # 학습 데이터 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0 \n",
      "  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터를 출력해보면 데이터가 0부터 255까지의 숫자(그레이스케일)로 구성된 것을 확인할 수 있다.\n",
    "print(y_train[:10])\n",
    "for i in x_train[0]:\n",
    "    for j in i:\n",
    "        print('%3d ' % j, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터를 불러왔으니 다층 퍼셉트론의 입력값으로 들어갈 수 있도록 numpy의 reshape 함수를 사용해 2차원 형태의 데이터를 1원 배열 형태로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 784)\n",
      "x_val.shape: (10000, 784)\n",
      "x_test.shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 28 * 28 픽셀의 단색 이미지이므로 데이터 형태를 784개의 1차원 배열 형태로 변환한다.\n",
    "import numpy as np\n",
    "x_train = np.reshape(x_train, (50000, 784)) # 28행 28열로 구성된 학습 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "x_val = x_val.reshape(10000, 784)           # 28행 28열로 구성된 검증 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_val.shape: {}'.format(x_val.shape))\n",
    "x_test = x_test.reshape(10000, 784)         # 28행 28열로 구성된 테스트 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_test.shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1차원으로 변경된 데이터를 그대로 다층 퍼셉트론에 입력해도 되지만 좀 더 효율적인 학습을 위해 데이터를 정규화 시킨다.  \n",
    "정규화는 모델의 학습 시간을 단축시키고 더 나은 성능을 보이게 하는 효과가 있다. MNIST 데이터의 모든 값은 0부터 255까지의 범위 안에 있으므로 255로 나눠 모든 값을 0부터 1사이의 값으로 정규화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train[0][0]), x_train[0][0])\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "# print(type(x_train[0][0]), x_train[0][0])\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 손글씨 데이터 분류 모델은 0부터 9까지의 숫자로 분류하는 다중 모델이므로 손실 함수로 크로스 엔트로피를 사용한다.  \n",
    "크로스 엔트로피를 계산하기 위해 실제값(y)은 원 핫 인코딩으로 변환한다.  \n",
    "원 핫 인코딩(One Hot Encoding)은 데이터를 수많은 0과 1개의 1값으로 데이터를 구별하는 인코딩 방식으로 0으로 이루어진 벡터 집합에 단 1개의 1의 값으로 해당 데이터를 구별하는 것을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# print(y_train[:5]) => [5 0 4 1 9]\n",
    "for i in y_train[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 데이터는 784개의 숫자가 들어있는 배열이다.  \n",
    "784개의 입력을 받는 256개의 노드가 1번째 레이어에 있고 1번째 레이어의 출력값을 입력으로 받는 2번째 레이어에는 128개의 노드가 있다. 2번째 레이어에 10% 드롭아웃을 적용하고 2번째 레이어의 출력을 받는 3번째 레이어에는 총 10개의 노드가 존재하며 이 10개의 노드값은 소프트맥스를 통과해서 0부터 9까지에 해당하는 각 숫자의 확률을 의미하게 된다.  \n",
    "소프트맥스 출력값과 실제값의 차이를 계산하기 위해 크로스 엔트로피를 손실 함수로 사용하고 손실 함수를 최소화하기 위해 Adam 옵티마이저를 사용해 역전파를 통해 모든 가중치 및 편향값을 최적화한다.  \n",
    "최적화 함수 참고 : https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam\n",
    "***\n",
    "소프트맥스는 분류해야하는 정답지(클래스)의 총 개수를 k라고 할 때 k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 차원이 None인 이유는 데이터 개수 제약없이 입력받기 위해서이고 \n",
    "# 2번째 차원이 784인 것은 MNIST의 이미지 크기가 28 * 28 픽셀 = 784 픽셀이기 때문이다.\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 입력값\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 출력값\n",
    "keep_prob = tf.placeholder(tf.float32)      # 드롭아웃 적용 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다층 퍼셉트론을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론을 구현한 함수\n",
    "def mlp(x):\n",
    "    # 히든 레이어1\n",
    "    w1 = tf.Variable(tf.random_uniform([784, 256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # 히든 레이어2\n",
    "    w2 = tf.Variable(tf.random_uniform([256, 128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    # 드롭아웃 적용, keep_prob만 썼었는데 버전이 올라가면서 rate=1-keep_prob 형태로 써야한다.\n",
    "    h2_drop = tf.nn.dropout(h2, rate=1-keep_prob)\n",
    "    # 히든 레이어3\n",
    "    w3 = tf.Variable(tf.random_uniform([128, 10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits = tf.nn.relu(tf.matmul(h2_drop, w3) + b3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론의 출력값을 logits로 정의한다.\n",
    "# logit => logistic과 probit의 합성어로 probit은 확률을 재는 단위라는 뜻이다.\n",
    "logits = mlp(x)\n",
    "# logits와 실제값의 크로스 엔트로피를 손실 함수로 사용한다.\n",
    "# 크로스 엔트로피 함수 공식 tf.reduce_mean(-tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)), reduction_indices=1))을 그대로\n",
    "# 사용하면 수치적으로 불안정하여 계산 오류가 발생할 수 있으므로\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "# Adam 옵티마이저를 사용해 모델을 최적화한다. 모델의 최적화 과정은 모델의 예측값과 실제값의 차이를 줄여나가는 과정을 의미한다.\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기 종료는 과대적합을 피하고 충분한 학습을 하기 위해 학습 중간마다 검증 데이터에 대한 정확도를 측정해 학습 정확도는 계속 증가하는 반면 검증 데이터에 대한 정확도가 점차 떨어질 경우 중지하는 것을 말한다.\n",
    "매 주기(epoch)마다 검증 데이터로 검증 정확도를 측정한다. 검증 정확도가 5번 연속으로 최고 검증 정확도 보다 높지 않을 경우 조기 종료를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # 텐서플로우 변수 초기화\n",
    "saver = tf.train.Saver()                 # 텐서플로우 모델의 저장 및 로드에 사용할 변수를 선언한다.\n",
    "epoch_cnt = 300                          # 조기 종료가 일어나지 않을 경우 최대 300번까지 반복해서 학습하도록 설정한다.\n",
    "batch_size = 1000                        # 1번에 처리할 데이터의 개수를 설정한다.\n",
    "iteration = len(x_train) // batch_size   # batch_size에 따른 학습 반복 횟수를 설정한다.\n",
    "earlystop_threshold = 5                  # 검증 정확도가 최고 정확도보다 5번 연속으로 높지 않을 경우 조기 종료하도록 설정한다.\n",
    "earlystop_cnt = 0                        # 검증 정확도가 최고 정확도보다 연속으로 높지 않은 횟수를 세는 변수를 선언한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행한다.  \n",
    "데이터를 모델에 입력할 때 모델에 드롭아웃이 적용된 경우 항상 keep_prob을 설정해야 한다. 학습 시 10%의 드롭아웃을 적용하기 위해 keep_drop을 0.9로 설정하고 테스트할 때는 드롭아웃을 적용하지 않을 겻이므로 keep_prob을 1.0으로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 0 2]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3, 10, 1]) # 1차원 배열\n",
    "sess = tf.Session()\n",
    "# tf.argmax() 메소드는 배열에서 가장 큰 값을 찾아서 그 인덱스를 리턴한다.\n",
    "# a 배열에서 10이 가장 크기 때문에 결과는 10의 인덱스인 1이 출력된다.\n",
    "print(sess.run(tf.argmax(a, 0)))\n",
    "\n",
    "b = tf.constant([[3, 10, 1], [4, 5, 6], [0, 8, 7]]) # 2차원 배열\n",
    "# tf.argmax() 메소드를 2차원 배열에서 실행할 때 2번째 인수로 0을 지정하면 각 열에서 가장 큰 값을 찾아 그 인덱스를 리턴한다.\n",
    "print(sess.run(tf.argmax(b, 0)))\n",
    "# tf.argmax() 메소드를 2차원 배열에서 실행할 때 2번째 인수로 1을 지정하면 각 행에서 가장 큰 값을 찾아 그 인덱스를 리턴한다.\n",
    "print(sess.run(tf.argmax(b, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0, 학습 정확도: 0.41432, 검증 정확도: 0.42220\n",
      "epoch:   1, 학습 정확도: 0.45506, 검증 정확도: 0.46270\n",
      "epoch:   2, 학습 정확도: 0.54442, 검증 정확도: 0.55290\n",
      "epoch:   3, 학습 정확도: 0.60650, 검증 정확도: 0.61330\n",
      "epoch:   4, 학습 정확도: 0.65248, 검증 정확도: 0.66200\n",
      "epoch:   5, 학습 정확도: 0.68636, 검증 정확도: 0.69800\n",
      "epoch:   6, 학습 정확도: 0.71306, 검증 정확도: 0.72240\n",
      "epoch:   7, 학습 정확도: 0.73680, 검증 정확도: 0.74540\n",
      "epoch:   8, 학습 정확도: 0.75672, 검증 정확도: 0.76590\n",
      "epoch:   9, 학습 정확도: 0.77548, 검증 정확도: 0.78500\n",
      "epoch:  10, 학습 정확도: 0.79128, 검증 정확도: 0.80010\n",
      "epoch:  11, 학습 정확도: 0.80554, 검증 정확도: 0.81360\n",
      "epoch:  12, 학습 정확도: 0.81884, 검증 정확도: 0.82500\n",
      "epoch:  13, 학습 정확도: 0.83100, 검증 정확도: 0.83450\n",
      "epoch:  14, 학습 정확도: 0.84230, 검증 정확도: 0.84450\n",
      "epoch:  15, 학습 정확도: 0.85232, 검증 정확도: 0.85430\n",
      "epoch:  16, 학습 정확도: 0.86112, 검증 정확도: 0.86050\n",
      "epoch:  17, 학습 정확도: 0.86848, 검증 정확도: 0.86740\n",
      "epoch:  18, 학습 정확도: 0.87542, 검증 정확도: 0.87330\n",
      "epoch:  19, 학습 정확도: 0.88080, 검증 정확도: 0.87810\n",
      "epoch:  20, 학습 정확도: 0.88556, 검증 정확도: 0.88040\n",
      "epoch:  21, 학습 정확도: 0.89032, 검증 정확도: 0.88770\n",
      "epoch:  22, 학습 정확도: 0.89536, 검증 정확도: 0.89190\n",
      "epoch:  23, 학습 정확도: 0.89992, 검증 정확도: 0.89520\n",
      "epoch:  24, 학습 정확도: 0.90360, 검증 정확도: 0.89840\n",
      "epoch:  25, 학습 정확도: 0.90584, 검증 정확도: 0.89880\n",
      "epoch:  26, 학습 정확도: 0.90834, 검증 정확도: 0.90270\n",
      "epoch:  27, 학습 정확도: 0.91126, 검증 정확도: 0.90490\n",
      "epoch:  28, 학습 정확도: 0.91426, 검증 정확도: 0.90780\n",
      "epoch:  29, 학습 정확도: 0.91814, 검증 정확도: 0.91110\n",
      "epoch:  30, 학습 정확도: 0.92058, 검증 정확도: 0.91380\n",
      "epoch:  31, 학습 정확도: 0.92360, 검증 정확도: 0.91570\n",
      "epoch:  32, 학습 정확도: 0.92540, 검증 정확도: 0.91700\n",
      "epoch:  33, 학습 정확도: 0.92728, 검증 정확도: 0.91910\n",
      "epoch:  34, 학습 정확도: 0.92882, 검증 정확도: 0.92060\n",
      "epoch:  35, 학습 정확도: 0.93104, 검증 정확도: 0.92110\n",
      "epoch:  36, 학습 정확도: 0.93302, 검증 정확도: 0.92410\n",
      "epoch:  37, 학습 정확도: 0.93482, 검증 정확도: 0.92440\n",
      "epoch:  38, 학습 정확도: 0.93622, 검증 정확도: 0.92570\n",
      "epoch:  39, 학습 정확도: 0.93810, 검증 정확도: 0.92730\n",
      "epoch:  40, 학습 정확도: 0.93958, 검증 정확도: 0.92860\n",
      "epoch:  41, 학습 정확도: 0.94106, 검증 정확도: 0.92890\n",
      "epoch:  42, 학습 정확도: 0.94238, 검증 정확도: 0.92970\n",
      "epoch:  43, 학습 정확도: 0.94374, 검증 정확도: 0.93120\n",
      "epoch:  44, 학습 정확도: 0.94452, 검증 정확도: 0.93080\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  45, 학습 정확도: 0.94506, 검증 정확도: 0.93200\n",
      "epoch:  46, 학습 정확도: 0.94652, 검증 정확도: 0.93310\n",
      "epoch:  47, 학습 정확도: 0.94752, 검증 정확도: 0.93330\n",
      "epoch:  48, 학습 정확도: 0.94812, 검증 정확도: 0.93380\n",
      "epoch:  49, 학습 정확도: 0.94948, 검증 정확도: 0.93340\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  50, 학습 정확도: 0.95036, 검증 정확도: 0.93330\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  51, 학습 정확도: 0.95058, 검증 정확도: 0.93580\n",
      "epoch:  52, 학습 정확도: 0.95174, 검증 정확도: 0.93510\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  53, 학습 정확도: 0.95232, 검증 정확도: 0.93670\n",
      "epoch:  54, 학습 정확도: 0.95394, 검증 정확도: 0.93680\n",
      "epoch:  55, 학습 정확도: 0.95490, 검증 정확도: 0.93710\n",
      "epoch:  56, 학습 정확도: 0.95568, 검증 정확도: 0.93800\n",
      "epoch:  57, 학습 정확도: 0.95646, 검증 정확도: 0.93920\n",
      "epoch:  58, 학습 정확도: 0.95728, 검증 정확도: 0.93900\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  59, 학습 정확도: 0.95742, 검증 정확도: 0.93970\n",
      "epoch:  60, 학습 정확도: 0.95850, 검증 정확도: 0.94060\n",
      "epoch:  61, 학습 정확도: 0.95922, 검증 정확도: 0.94080\n",
      "epoch:  62, 학습 정확도: 0.96014, 검증 정확도: 0.94110\n",
      "epoch:  63, 학습 정확도: 0.96058, 검증 정확도: 0.94280\n",
      "epoch:  64, 학습 정확도: 0.96112, 검증 정확도: 0.94260\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  65, 학습 정확도: 0.96108, 검증 정확도: 0.94200\n",
      "epoch:  66, 학습 정확도: 0.96314, 검증 정확도: 0.94330\n",
      "epoch:  67, 학습 정확도: 0.96312, 검증 정확도: 0.94280\n",
      "epoch:  68, 학습 정확도: 0.96378, 검증 정확도: 0.94320\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  69, 학습 정확도: 0.96412, 검증 정확도: 0.94380\n",
      "epoch:  70, 학습 정확도: 0.96446, 검증 정확도: 0.94260\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  71, 학습 정확도: 0.96510, 검증 정확도: 0.94380\n",
      "epoch:  72, 학습 정확도: 0.96570, 검증 정확도: 0.94290\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  73, 학습 정확도: 0.96628, 검증 정확도: 0.94370\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  74, 학습 정확도: 0.96588, 검증 정확도: 0.94250\n",
      "epoch:  75, 학습 정확도: 0.96814, 검증 정확도: 0.94340\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  76, 학습 정확도: 0.96706, 검증 정확도: 0.94250\n",
      "epoch:  77, 학습 정확도: 0.96796, 검증 정확도: 0.94290\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  78, 학습 정확도: 0.96832, 검증 정확도: 0.94370\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  79, 학습 정확도: 0.96872, 검증 정확도: 0.94280\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  80, 학습 정확도: 0.96986, 검증 정확도: 0.94340\n",
      "과대적합 경고 횟수: 3\n",
      "epoch:  81, 학습 정확도: 0.97046, 검증 정확도: 0.94340\n",
      "과대적합 경고 횟수: 4\n",
      "epoch:  82, 학습 정확도: 0.96988, 검증 정확도: 0.94260\n",
      "epoch:  83, 학습 정확도: 0.96822, 검증 정확도: 0.94220\n",
      "epoch:  84, 학습 정확도: 0.97064, 검증 정확도: 0.94270\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  85, 학습 정확도: 0.96984, 검증 정확도: 0.94360\n",
      "epoch:  86, 학습 정확도: 0.97072, 검증 정확도: 0.94510\n",
      "epoch:  87, 학습 정확도: 0.97184, 검증 정확도: 0.94530\n",
      "epoch:  88, 학습 정확도: 0.97216, 검증 정확도: 0.94520\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  89, 학습 정확도: 0.97304, 검증 정확도: 0.94480\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  90, 학습 정확도: 0.97302, 검증 정확도: 0.94590\n",
      "epoch:  91, 학습 정확도: 0.97472, 검증 정확도: 0.94640\n",
      "epoch:  92, 학습 정확도: 0.97532, 검증 정확도: 0.94670\n",
      "epoch:  93, 학습 정확도: 0.97476, 검증 정확도: 0.94390\n",
      "epoch:  94, 학습 정확도: 0.97476, 검증 정확도: 0.94530\n",
      "epoch:  95, 학습 정확도: 0.97582, 검증 정확도: 0.94520\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  96, 학습 정확도: 0.97410, 검증 정확도: 0.94520\n",
      "epoch:  97, 학습 정확도: 0.97464, 검증 정확도: 0.94530\n",
      "과대적합 경고 횟수: 0\n",
      "epoch:  98, 학습 정확도: 0.97364, 검증 정확도: 0.94490\n",
      "epoch:  99, 학습 정확도: 0.97484, 검증 정확도: 0.94620\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 100, 학습 정확도: 0.97318, 검증 정확도: 0.94500\n",
      "epoch: 101, 학습 정확도: 0.97488, 검증 정확도: 0.94670\n",
      "epoch: 102, 학습 정확도: 0.97334, 검증 정확도: 0.94580\n",
      "epoch: 103, 학습 정확도: 0.97816, 검증 정확도: 0.94770\n",
      "epoch: 104, 학습 정확도: 0.97658, 검증 정확도: 0.94630\n",
      "epoch: 105, 학습 정확도: 0.97736, 검증 정확도: 0.94680\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 106, 학습 정확도: 0.97770, 검증 정확도: 0.94690\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 107, 학습 정확도: 0.97888, 검증 정확도: 0.94840\n",
      "epoch: 108, 학습 정확도: 0.97878, 검증 정확도: 0.94880\n",
      "epoch: 109, 학습 정확도: 0.97982, 검증 정확도: 0.94690\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 110, 학습 정확도: 0.97870, 검증 정확도: 0.94600\n",
      "epoch: 111, 학습 정확도: 0.97988, 검증 정확도: 0.94820\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 112, 학습 정확도: 0.97956, 검증 정확도: 0.94830\n",
      "epoch: 113, 학습 정확도: 0.98178, 검증 정확도: 0.94950\n",
      "epoch: 114, 학습 정확도: 0.97976, 검증 정확도: 0.94860\n",
      "epoch: 115, 학습 정확도: 0.98054, 검증 정확도: 0.94970\n",
      "epoch: 116, 학습 정확도: 0.97892, 검증 정확도: 0.94800\n",
      "epoch: 117, 학습 정확도: 0.97988, 검증 정확도: 0.94750\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 118, 학습 정확도: 0.97972, 검증 정확도: 0.94870\n",
      "epoch: 119, 학습 정확도: 0.98062, 검증 정확도: 0.94870\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 120, 학습 정확도: 0.98190, 검증 정확도: 0.94890\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 121, 학습 정확도: 0.98070, 검증 정확도: 0.94820\n",
      "epoch: 122, 학습 정확도: 0.98404, 검증 정확도: 0.95140\n",
      "epoch: 123, 학습 정확도: 0.98330, 검증 정확도: 0.95110\n",
      "epoch: 124, 학습 정확도: 0.97878, 검증 정확도: 0.94570\n",
      "epoch: 125, 학습 정확도: 0.98352, 검증 정확도: 0.95060\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 126, 학습 정확도: 0.98200, 검증 정확도: 0.94880\n",
      "epoch: 127, 학습 정확도: 0.98250, 검증 정확도: 0.95110\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 128, 학습 정확도: 0.98180, 검증 정확도: 0.95070\n",
      "epoch: 129, 학습 정확도: 0.98276, 검증 정확도: 0.95160\n",
      "epoch: 130, 학습 정확도: 0.97948, 검증 정확도: 0.94740\n",
      "epoch: 131, 학습 정확도: 0.98134, 검증 정확도: 0.94900\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 132, 학습 정확도: 0.98046, 검증 정확도: 0.94950\n",
      "epoch: 133, 학습 정확도: 0.98480, 검증 정확도: 0.95310\n",
      "epoch: 134, 학습 정확도: 0.98242, 검증 정확도: 0.94870\n",
      "epoch: 135, 학습 정확도: 0.98152, 검증 정확도: 0.95180\n",
      "epoch: 136, 학습 정확도: 0.98254, 검증 정확도: 0.95200\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 137, 학습 정확도: 0.97988, 검증 정확도: 0.94820\n",
      "epoch: 138, 학습 정확도: 0.98286, 검증 정확도: 0.95070\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 139, 학습 정확도: 0.98230, 검증 정확도: 0.95010\n",
      "epoch: 140, 학습 정확도: 0.98376, 검증 정확도: 0.95190\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 141, 학습 정확도: 0.98394, 검증 정확도: 0.95060\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 142, 학습 정확도: 0.98108, 검증 정확도: 0.95210\n",
      "epoch: 143, 학습 정확도: 0.98564, 검증 정확도: 0.95450\n",
      "epoch: 144, 학습 정확도: 0.98408, 검증 정확도: 0.95140\n",
      "epoch: 145, 학습 정확도: 0.98474, 검증 정확도: 0.95200\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 146, 학습 정확도: 0.98602, 검증 정확도: 0.95420\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 147, 학습 정확도: 0.98636, 검증 정확도: 0.95560\n",
      "epoch: 148, 학습 정확도: 0.98306, 검증 정확도: 0.95290\n",
      "epoch: 149, 학습 정확도: 0.98504, 검증 정확도: 0.95420\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 150, 학습 정확도: 0.98590, 검증 정확도: 0.95280\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 151, 학습 정확도: 0.98700, 검증 정확도: 0.95470\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 152, 학습 정확도: 0.98660, 검증 정확도: 0.95580\n",
      "epoch: 153, 학습 정확도: 0.98572, 검증 정확도: 0.95420\n",
      "epoch: 154, 학습 정확도: 0.98824, 검증 정확도: 0.95500\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 155, 학습 정확도: 0.98910, 검증 정확도: 0.95570\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 156, 학습 정확도: 0.98730, 검증 정확도: 0.95390\n",
      "epoch: 157, 학습 정확도: 0.98634, 검증 정확도: 0.95500\n",
      "epoch: 158, 학습 정확도: 0.98660, 검증 정확도: 0.95350\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 159, 학습 정확도: 0.98710, 검증 정확도: 0.95620\n",
      "epoch: 160, 학습 정확도: 0.98744, 검증 정확도: 0.95560\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 161, 학습 정확도: 0.98560, 검증 정확도: 0.95490\n",
      "epoch: 162, 학습 정확도: 0.98572, 검증 정확도: 0.95500\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 163, 학습 정확도: 0.98686, 검증 정확도: 0.95410\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 164, 학습 정확도: 0.98508, 검증 정확도: 0.95400\n",
      "epoch: 165, 학습 정확도: 0.98982, 검증 정확도: 0.95820\n",
      "epoch: 166, 학습 정확도: 0.98894, 검증 정확도: 0.95450\n",
      "epoch: 167, 학습 정확도: 0.99064, 검증 정확도: 0.95650\n",
      "과대적합 경고 횟수: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 168, 학습 정확도: 0.98916, 검증 정확도: 0.95580\n",
      "epoch: 169, 학습 정확도: 0.98900, 검증 정확도: 0.95630\n",
      "epoch: 170, 학습 정확도: 0.98944, 검증 정확도: 0.95650\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 171, 학습 정확도: 0.98846, 검증 정확도: 0.95460\n",
      "epoch: 172, 학습 정확도: 0.99044, 검증 정확도: 0.95490\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 173, 학습 정확도: 0.98814, 검증 정확도: 0.95700\n",
      "epoch: 174, 학습 정확도: 0.99026, 검증 정확도: 0.95550\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 175, 학습 정확도: 0.98802, 검증 정확도: 0.95600\n",
      "epoch: 176, 학습 정확도: 0.98888, 검증 정확도: 0.95700\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 177, 학습 정확도: 0.98686, 검증 정확도: 0.95610\n",
      "epoch: 178, 학습 정확도: 0.98988, 검증 정확도: 0.95620\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 179, 학습 정확도: 0.99088, 검증 정확도: 0.95780\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 180, 학습 정확도: 0.98914, 검증 정확도: 0.95450\n",
      "epoch: 181, 학습 정확도: 0.98998, 검증 정확도: 0.95770\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 182, 학습 정확도: 0.99102, 검증 정확도: 0.95760\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 183, 학습 정확도: 0.99056, 검증 정확도: 0.95900\n",
      "epoch: 184, 학습 정확도: 0.99172, 검증 정확도: 0.95810\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 185, 학습 정확도: 0.98972, 검증 정확도: 0.95670\n",
      "epoch: 186, 학습 정확도: 0.98974, 검증 정확도: 0.95510\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 187, 학습 정확도: 0.98892, 검증 정확도: 0.95540\n",
      "epoch: 188, 학습 정확도: 0.98992, 검증 정확도: 0.95630\n",
      "과대적합 경고 횟수: 0\n",
      "epoch: 189, 학습 정확도: 0.99008, 검증 정확도: 0.95780\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 190, 학습 정확도: 0.99146, 검증 정확도: 0.95510\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 191, 학습 정확도: 0.99104, 검증 정확도: 0.95720\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 192, 학습 정확도: 0.99090, 검증 정확도: 0.95550\n",
      "과대적합 경고 횟수: 4\n",
      "epoch: 193, 학습 정확도: 0.99134, 검증 정확도: 0.95800\n",
      "조기 종료 시점: 193\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)       # 텐서플로우 변수 초기화\n",
    "    prev_train_acc = 0.0 # 이전 학습 정확도를 기억하는 변수를 선언한다.\n",
    "    max_val_acc = 0.0    # 검증 정확도의 최대값을 기억하는 변수를 선언한다.\n",
    "    # 지정된 최대 epoch 만큼 학습한다. => 검증 정확도가 최고 정확도보다 5번 연속으로 높지 않을 경우 학습을 조기 종료한다.\n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.0   # 손실값\n",
    "        start = 0        # 학습 시작 위치\n",
    "        end = batch_size # 학습 끝 위치\n",
    "        # 학습 데이터를 batch_size 개씩 나눠 학습을 진행한다.\n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], feed_dict={x: x_train[start:end], y: y_train[start:end], keep_prob:0.9})\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            # 크로스 엔트로피를 손실 함수의 학습 손실값을 계산한다.\n",
    "            avg_loss += loss / iteration\n",
    "        # =================================================\n",
    "        \n",
    "        # 모델 검증\n",
    "        preds = tf.nn.softmax(logits) # 소프트맥스 적용\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        # 정확도 계산\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        # 학습 정확도 계산\n",
    "        # Session.run()과 Tensor.eval()의 차이\n",
    "        # t가 Tensor 오브젝트라면 t.eval()은 sess.run(t)의 속기 표현이다. (sess가 현재 디폴트 세션인 곳에서만)\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train, keep_prob: 1.0})\n",
    "        # 검증 정확도 계산\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # 검증 데이터에 대한 손실값 계산\n",
    "        cur_val_loss = loss_op.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # 학습 정확도와 검증 정확도를 출력한다.\n",
    "        print('epoch: {0:3d}, 학습 정확도: {1:7.5f}, 검증 정확도: {2:7.5f}'.format(epoch, cur_train_acc, cur_val_acc))\n",
    "        \n",
    "        # 현재 검증 정확도와 검증 정확도의 최대값을 비교한다.\n",
    "        if cur_val_acc < max_val_acc:\n",
    "            # 현재 검증 정확도가 검증 정확도의 최대값 미만이면\n",
    "            # 현재 학습 정확도와 이전 학습 정확도, 학습 정확도와 0.99를 비교한다.\n",
    "            if (cur_train_acc > prev_train_acc) or (cur_train_acc > 0.99):\n",
    "                # 현재 학습 정확도가 이전 학습 정확도 보다 크거나 현재 학습 정확도가 0.99 보다 크면\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    # 5번 연속으로 현재 학습 정확도가 이전 학습 정확도 보다 크면 조기 종료한다.\n",
    "                    print('조기 종료 시점: {}'.format(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    # 5번 연속으로 현재 학습 정확도가 이전 학습 정확도 보다 크지 않다면 이전 학습 정확도가\n",
    "                    # 현재 학습 정확도보다 크므로 현재 검증 정확도가 최고 정확도의 최대값보다 \n",
    "                    # 연속으로 높지 않은 횟수를 카운트 하는 변수를 1 증가 시킨다.\n",
    "                    print('과대적합 경고 횟수: {}'.format(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "                # =================================================\n",
    "            else:\n",
    "                # 현재 학습 정확도가 이전 학습 정확도 보다 작거나, 현재 학습 정확도가 0.99 이하면\n",
    "                # 이전 학습 정확도가 현재 학습 정확도 이상이므로 현재 검증 정확도가 정확도의 최대값 보다 연속으로 높지 않은\n",
    "                # 횟수를 카운트하는 변수를 다시 0으로 초기화 시킨다.\n",
    "                earlystop_cnt = 0\n",
    "            # ===============================================\n",
    "        else:\n",
    "            # 현재 검증 정확도가 검증 정확도의 최대값 이상이면\n",
    "            # 현재 검증 정확도가 검증 정확도의 최대값 보다 연속으로 높지 않은 횟수를 카운트하는 변수를\n",
    "            # 다시 0으로 초기화 시킨다.\n",
    "            earlystop_cnt = 0\n",
    "            # 검증 정확도의 최대값을 현재 검증 정확도로 교체한다.\n",
    "            max_val_acc = cur_val_acc\n",
    "            # 검증 정확도가 가장 높은 모델을 저장한다.\n",
    "            save_path = saver.save(sess, './model/model.ckpt')\n",
    "        # ===============================================\n",
    "        \n",
    "        # 다음 학습을 위해 현재 학습 정확도를 이전 학습 정확도를 기억하는 변수에 넣어준다.\n",
    "        prev_train_acc = cur_train_acc\n",
    "    # ==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
      "정확도: 0.990559995174408\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 검증 결과가 가장 높았던 모델을 불러온다.\n",
    "    saver.restore(sess, './model/model.ckpt')\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # 정확도 계산\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print('정확도: {}'.format(accuracy.eval({x: x_train, y: y_train, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
